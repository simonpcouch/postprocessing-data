---
title: "Data Usage with Postprocessing"
author: "Max Kuhn"
---


## Introduction

This document is used to discuss and test ideas for how have can estimate and evaluate machine learning models that have three potential components: 

- _Preprocessors_ are sequential operations that prepare predictor data for use in a supervised model. Examples are centering/scaling, PCA feature extraction, and so on. 
- A _supervised ML model_ to translate the predictors to predictions of the outcome (e.g., logistic regression, random forest, etc.).
- _Postprocessors_ that take the model's predictions and change them for the purpose of improving model performance. One example is choosing an alternate probability cutoff in binary classification to optimize for better true positive or true negative rates. 

We'll call the combination of these three components the _model pipeline_ (taking after python's sci-kit learn object type). The pipeline includes the model in the least; the pre and post model operations are added as needed. 

## More About Postprocessors

The process of postprocessing the predicted values has not been thoroughly discussed, mostly because the software to operationalize the full pipeline process is not comprehensive. 

The number of potential postprocessors is probably in single digits. Some examples: 

* Specifying an optimal probability cutoff (mentioned above).
* Restricting the range of possible predictions (e.g., geater than zero). 
* Simple deterministic transformations (e.g., exponentiation). 
* Disqualifying predictions via an [equivocal zone](https://blog.aml4td.org/posts/equivocal-zones/index.html).
* Post-hoc nearest neighbor adjustments such as [Quinlan (1993)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Combining+instance-based+and+model-based+learning&btnG=).
* Calibration. 

Each of these steps can involve tuning parameters that require optimization. For example, we can vary the cutoff value over a range for alternate probability cutoffs and measure the performance change in some statistic that uses the hard class predictions (such as accuracy or Kappa). These types of parameters are estimated _indirectly_ via gris search or some other tuning parameter optimization routine. There is no analytical formula where we plug in our predictor and outcome data to produce a point estimate to plug into the postprocessor.

However, the last two in the list above might also have parameter values that require direct estimation (akin to slope parameters in a linear regression). Of these, let's focus on _model calibration_ for more discussion.  

MORE HERE

In the examples that follow, we'll visualize the data spending schemes with an initial pool of 100 samples, assumed to be in a random order:

```{r}
#| label: pool
#| echo: false
#| fig-align: "center"
#| out-width: "50%"

knitr::include_graphics("premade/sample-pool.svg")
```

Let's start with the ordinary case where we use a simple two-way training/testing split of the data, then consider different analysis paths.

## Initial Two-Way Split

There are a few different scenarios to consider. The first two are very pedestrian and are only included to contrast with the more complex ones. 

### Case 1: No Tuning, No Postprocessing Estimation

This is a simple case where a basic model will suffice with tuning parameters and no uncertainty about what predictors should be in the model. 

"No Postprocessing Estimation" means that there might be a postprocessor but it does not require any parameter estimation. For example, it might just change the probability cutoff for a binary classification to be something other than 50%.

We split the data into a larger set (training, in orange), and the remainder goes into testing (purple). Any 80/20 split is used to demonstrate:

```{r}
#| label: initial-split
#| echo: false
#| fig-align: "center"
#| out-width: "50%"

knitr::include_graphics("premade/no-estimation-split.svg")
```

All of our estimation tasks use the training set and the test set is evaluated only once to quantify the efficacy of the model.  

### Case 2: Tuning, No Postprocessing Estimation

Here, some aspects of the model, preprocessor, or postprocessor required optimization. Any postprocessor does not require estimation (but could require tuning). 

Using the same initial split from Case 1, we might use some resampling strategy like cross-validation, the bootstrap, or a time-series resampling scheme. Without loss of generalization, we'll show a 5-fold cross-validation diagram: 


```{r}
#| label: basic-cv
#| echo: false
#| fig-align: "center"
#| out-width: "45%"

knitr::include_graphics("premade/training-to-5-fold.svg")
```

As usual, we fit five model pipelines with different tuning parameter candidates. Each model uses 4/5 of the data for estimation and the remaining 1/5 to measure performance. The resulting five performance statistics are averaged into a single value, which is used to guide the user in picking which tuning parameters are optimal (or at least reasonable). 

Once the optimization phase is finished, the final model uses the optimized tuning parameter values and is fit to the 80 data points of the training set. The other 20 samples in the test set are used to verify performance. 

Note that tidymodels uses specific terminology to distinguish between the data used for modeling and evaluation at the two different levels of data partitioning. The initial split creates training and test sets. During resampling, the analogs to these data sets are called the _analysis_ and _assessment_ sets. 

### Case 3: No Tuning, Postprocessing Estimation

Here our model pipeline requires no tuning but we do need to estimate parameters for our postprocessor. 

For example, perhaps our ordinary least squares linear regression model has some systematic bias in the predictions (and we have to use this model). We could attach a linear calibrator to the model pipeline that estimates the slope and intercept of a line defined by the observed and predicted outcomes (as shown above). 

We need data to estimate the slope and intercept. We should not touch the test set. Naively re-predicting the training set is a poor choice; for many black-box models, the fitted values will be unreasonably close to the true values. This means that the systematic bias that we are trying to remove will be less pronounced and the calibration may not help. It also leaves us with no other data to judge how well the model (and calibration)  works without using the test set. 

One possible approach is to resample the model (prior to the calibration set) using the approach in Case 2. This can produce the out-of-sample predictions that were used to produce the resampled performance statistic. These values are not overfit to the training data and should be a reasonable substrate to fit the calibration model. The main downside to this approach is that we are "double dipping" on the training set but using it to 

1. Estimate our model parameters, and 
2. Estimate the calibration parameters. 

This raises the risk of overfitting and we don't have a data set  to check how well this works until the test set (which should be used to verify performance). 

One approach is to use a three-way split at the start instead of a basic training/test set. We could reserve some data strictly for calibration (assuming that we know that calibration is required). 

We can allocate a small fraction of data for postprocessing estimation. A diagram of this is before with 60% used for training the preprocessor and supervised model, 20% for estimating the postprocessor, and 20% for testing^[Again, we don't have to do this for all postprocessors, just those that require parameters to be estimated]. In the diagram below, the two shades of brown are meant to reflect that these data are used for estimation and the purple data are used strictly for model evaluation. 

```{r}
#| label: initial-reserve
#| echo: false
#| fig-align: "center"
#| out-width: "45%"

knitr::include_graphics("premade/with-estimation-split.svg")
```

The nomenclature is a little bit fuzzy here. For now, we'll call the darker brown data the training set (no different than before), the purple data the test set, and the light brown data the "potato set"^[Obviously, this is not going to be the real name. We need a placeholder until we come up with something that we all like. Potential candidate names are the "reserved data," auxiliary data," and "supplemental data."]

This extra set is a simple solution that avoids potential data leakage but is reducing the amount of data used to train the preprocessors and the supervised model. 

The next use-case is for situations where the model needs to be resampled for tuning or just to get some estimate of model performance. 

### Case 4: Tuning, Postprocessing Estimation

Now our model and/or preprocessor have unknown parameters that need to be indirectly optimized via grid search, Bayesian optimization, or by some other means. The compare and choose between models, we require an out-of-sample performance estimate, just as in Case 2. 

The difference here is the existence of a postprocessor that needs estimation. 

Once we arrive at our final tuning parameter value(s), we still need to perform the "last fit" where we estimate all of the parameters for the entire model pipeline. 

Let's say we use the three-way data splitting scheme shown above in Case 3. How do we resample the model? We suggest taking _all_ data that are not used for the training set as the substrate for resampling. Let's again use 5-fold cross-validation to demonstrate. The 80 samples are allocated to one of five folds. 

```{r}
#| label: initial-reserve-again
#| echo: false
#| fig-align: "center"
#| out-width: "45%"

knitr::include_graphics("premade/training-reserve-to-5-fold.svg")
```

For the first iteration of cross-validation, we take out the first fold earmarked for performance estimation as the assessment set. 

Ordinarily, the other 4/5 would be used to estimate the preprocessor(s) and the model. However, we definitely need to include the postprocessor's effect within resampling; otherwise, our resampling performance statistics will have overly optimistic values. 

We can emulate the same procedure used in our initial three-way split by randomly^[_Most of the time_, this will be done via random sampling. For time-series data, we would emulate the same non-random splitting strategy that does not break the correlation structure of the data. Also, if we are bootstrapping, the proportional splits are conducted on the distinct rows of the non-test data to avoid having some replicates of specific rows falling in both partitions of the data.] selecting the same proportion of data to estimate the two estimation stages.

Visually, the scheme for the first iteration of cross-validation is: 

```{r}
#| label: split-within-cv
#| echo: false
#| fig-align: "center"
#| out-width: "65%"

knitr::include_graphics("premade/reserve-in-cv-iter-1.svg")
```

In this instance, five preprocessor/model fits are paired with five calibration models, and when combined in sequence, they produce five resampled performance statistics. This is a complete resampling of the process that avoids information leakage.    

