[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Usage with Postprocessing",
    "section": "",
    "text": "This document is used to discuss and test ideas for how have can estimate and evaluate machine learning models that have three potential components:\n\nPreprocessors are sequential operations that prepare predictor data for use in a supervised model. Examples are centering/scaling, PCA feature extraction, and so on.\nA supervised ML model to translate the predictors to predictions of the outcome (e.g., logistic regression, random forest, etc.).\nPostprocessors that take the model’s predictions and change them for the purpose of improving model performance. One example is choosing an alternate probability cutoff in binary classification to optimize for better true positive or true negative rates.\n\nWe’ll call the combination of these three components the model pipeline (taking after python’s sci-kit learn object type). The pipeline includes the model in the least; the pre and post model operations are added as needed."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Data Usage with Postprocessing",
    "section": "",
    "text": "This document is used to discuss and test ideas for how have can estimate and evaluate machine learning models that have three potential components:\n\nPreprocessors are sequential operations that prepare predictor data for use in a supervised model. Examples are centering/scaling, PCA feature extraction, and so on.\nA supervised ML model to translate the predictors to predictions of the outcome (e.g., logistic regression, random forest, etc.).\nPostprocessors that take the model’s predictions and change them for the purpose of improving model performance. One example is choosing an alternate probability cutoff in binary classification to optimize for better true positive or true negative rates.\n\nWe’ll call the combination of these three components the model pipeline (taking after python’s sci-kit learn object type). The pipeline includes the model in the least; the pre and post model operations are added as needed."
  },
  {
    "objectID": "index.html#more-about-postprocessors",
    "href": "index.html#more-about-postprocessors",
    "title": "Data Usage with Postprocessing",
    "section": "More About Postprocessors",
    "text": "More About Postprocessors\nThe process of postprocessing the predicted values has not been thoroughly discussed, mostly because the software to operationalize the full pipeline process is not comprehensive.\nThe number of potential postprocessors is probably in single digits. Some examples:\n\nSpecifying an optimal probability cutoff (mentioned above).\nRestricting the range of possible predictions (e.g., geater than zero).\nSimple deterministic transformations (e.g., exponentiation).\nDisqualifying predictions via an equivocal zone.\nPost-hoc nearest neighbor adjustments such as Quinlan (1993).\nCalibration.\n\nEach of these steps can involve tuning parameters that require optimization. For example, we can vary the cutoff value over a range for alternate probability cutoffs and measure the performance change in some statistic that uses the hard class predictions (such as accuracy or Kappa). These types of parameters are estimated indirectly via gris search or some other tuning parameter optimization routine. There is no analytical formula where we plug in our predictor and outcome data to produce a point estimate to plug into the postprocessor.\nHowever, the last two in the list above might also have parameter values that require direct estimation (akin to slope parameters in a linear regression). Of these, let’s focus on model calibration for more discussion.\nMORE HERE\nIn the examples that follow, we’ll visualize the data spending schemes with an initial pool of 100 samples, assumed to be in a random order:\n\n\n\n\n\n\n\n\n\nLet’s start with the ordinary case where we use a simple two-way training/testing split of the data, then consider different analysis paths."
  },
  {
    "objectID": "index.html#initial-two-way-split",
    "href": "index.html#initial-two-way-split",
    "title": "Data Usage with Postprocessing",
    "section": "Initial Two-Way Split",
    "text": "Initial Two-Way Split\nThere are a few different scenarios to consider. The first two are very pedestrian and are only included to contrast with the more complex ones.\n\nCase 1: No Tuning, No Postprocessing Estimation\nThis is a simple case where a basic model will suffice with tuning parameters and no uncertainty about what predictors should be in the model.\n“No Postprocessing Estimation” means that there might be a postprocessor but it does not require any parameter estimation. For example, it might just change the probability cutoff for a binary classification to be something other than 50%.\nWe split the data into a larger set (training, in orange), and the remainder goes into testing (purple). Any 80/20 split is used to demonstrate:\n\n\n\n\n\n\n\n\n\nAll of our estimation tasks use the training set and the test set is evaluated only once to quantify the efficacy of the model.\n\n\nCase 2: Tuning, No Postprocessing Estimation\nHere, some aspects of the model, preprocessor, or postprocessor required optimization. Any postprocessor does not require estimation (but could require tuning).\nUsing the same initial split from Case 1, we might use some resampling strategy like cross-validation, the bootstrap, or a time-series resampling scheme. Without loss of generalization, we’ll show a 5-fold cross-validation diagram:\n\n\n\n\n\n\n\n\n\nAs usual, we fit five model pipelines with different tuning parameter candidates. Each model uses 4/5 of the data for estimation and the remaining 1/5 to measure performance. The resulting five performance statistics are averaged into a single value, which is used to guide the user in picking which tuning parameters are optimal (or at least reasonable).\nOnce the optimization phase is finished, the final model uses the optimized tuning parameter values and is fit to the 80 data points of the training set. The other 20 samples in the test set are used to verify performance.\nNote that tidymodels uses specific terminology to distinguish between the data used for modeling and evaluation at the two different levels of data partitioning. The initial split creates training and test sets. During resampling, the analogs to these data sets are called the analysis and assessment sets.\n\n\nCase 3: No Tuning, Postprocessing Estimation\nHere our model pipeline requires no tuning but we do need to estimate parameters for our postprocessor.\nFor example, perhaps our ordinary least squares linear regression model has some systematic bias in the predictions (and we have to use this model). We could attach a linear calibrator to the model pipeline that estimates the slope and intercept of a line defined by the observed and predicted outcomes (as shown above).\nWe need data to estimate the slope and intercept. We should not touch the test set. Naively re-predicting the training set is a poor choice; for many black-box models, the fitted values will be unreasonably close to the true values. This means that the systematic bias that we are trying to remove will be less pronounced and the calibration may not help. It also leaves us with no other data to judge how well the model (and calibration) works without using the test set.\nOne possible approach is to resample the model (prior to the calibration set) using the approach in Case 2. This can produce the out-of-sample predictions that were used to produce the resampled performance statistic. These values are not overfit to the training data and should be a reasonable substrate to fit the calibration model. The main downside to this approach is that we are “double dipping” on the training set but using it to\n\nEstimate our model parameters, and\nEstimate the calibration parameters.\n\nThis raises the risk of overfitting and we don’t have a data set to check how well this works until the test set (which should be used to verify performance).\nOne approach is to use a three-way split at the start instead of a basic training/test set. We could reserve some data strictly for calibration (assuming that we know that calibration is required).\nWe can allocate a small fraction of data for postprocessing estimation. A diagram of this is before with 60% used for training the preprocessor and supervised model, 20% for estimating the postprocessor, and 20% for testing1. In the diagram below, the two shades of brown are meant to reflect that these data are used for estimation and the purple data are used strictly for model evaluation.\n\n\n\n\n\n\n\n\n\nThe nomenclature is a little bit fuzzy here. For now, we’ll call the darker brown data the training set (no different than before), the purple data the test set, and the light brown data the “potato set”2\nThis extra set is a simple solution that avoids potential data leakage but is reducing the amount of data used to train the preprocessors and the supervised model.\nThe next use-case is for situations where the model needs to be resampled for tuning or just to get some estimate of model performance.\n\n\nCase 4: Tuning, Postprocessing Estimation\nNow our model and/or preprocessor have unknown parameters that need to be indirectly optimized via grid search, Bayesian optimization, or by some other means. The compare and choose between models, we require an out-of-sample performance estimate, just as in Case 2.\nThe difference here is the existence of a postprocessor that needs estimation.\nOnce we arrive at our final tuning parameter value(s), we still need to perform the “last fit” where we estimate all of the parameters for the entire model pipeline.\nLet’s say we use the three-way data splitting scheme shown above in Case 3. How do we resample the model? We suggest taking all data that are not used for the training set as the substrate for resampling. Let’s again use 5-fold cross-validation to demonstrate. The 80 samples are allocated to one of five folds.\n\n\n\n\n\n\n\n\n\nFor the first iteration of cross-validation, we take out the first fold earmarked for performance estimation as the assessment set.\nOrdinarily, the other 4/5 would be used to estimate the preprocessor(s) and the model. However, we definitely need to include the postprocessor’s effect within resampling; otherwise, our resampling performance statistics will have overly optimistic values.\nWe can emulate the same procedure used in our initial three-way split by randomly3 selecting the same proportion of data to estimate the two estimation stages.\nVisually, the scheme for the first iteration of cross-validation is:\n\n\n\n\n\n\n\n\n\nIn this instance, five preprocessor/model fits are paired with five calibration models, and when combined in sequence, they produce five resampled performance statistics. This is a complete resampling of the process that avoids information leakage."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Data Usage with Postprocessing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAgain, we don’t have to do this for all postprocessors, just those that require parameters to be estimated↩︎\nObviously, this is not going to be the real name. We need a placeholder until we come up with something that we all like. Potential candidate names are the “reserved data,” auxiliary data,” and “supplemental data.”↩︎\nMost of the time, this will be done via random sampling. For time-series data, we would emulate the same non-random splitting strategy that does not break the correlation structure of the data. Also, if we are bootstrapping, the proportional splits are conducted on the distinct rows of the non-test data to avoid having some replicates of specific rows falling in both partitions of the data.↩︎"
  }
]